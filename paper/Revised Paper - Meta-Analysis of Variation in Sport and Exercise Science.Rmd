---
title: "Meta-Analysis of Variation in Sport and Exercise Science"
subtitle: "Examples of Application Within Resistance Training Research"
author: 
  - James Steele$^a$
  - James Fisher$^a$
  - Dave Smith$^b$
  - Brad Schoenfeld$^c$
  - Yefeng Yang$^d$
  - Shinichi Nakagawa$^d$
date: "14th September 2023"
abstract: "Meta-analysis has become commonplace within sport and exercise science for synthesising and summarising empirical studies. However, most research in the field focuses upon mean effects; particularly the effects of interventions to improve outcomes such as fitness or performance. It is thought that individual responses to interventions vary considerably. Hence, interest has increased in exploring *precision* or *personalised* exercise approaches. Not only is the mean often affected by interventions, but variances may also be impacted. Exploration of variances in studies such as randomised controlled trials (RCTs) can yield insight into interindividual heterogeneity in response to interventions and help determine generalisability of effects. Yet, larger samples sizes than those used for typical mean effects are required when probing variances. Thus, in a field with small samples such as sport and exercise science, exploration of variance through a meta-analytic framework is appealing. Despite the value of embracing and exploring variation alongside mean effects in sport and exercise science it is rarely applied to research synthesis through meta-analysis.We introduce and evaluate different effect size calculations along with models for meta-analysis of variation using relatable examples from resistance training RCTs."
thanks: |
  Address for correspondence: james.steele@solent.ac.uk
  Affiliations:
  $^a$Faculty of Sport, Health, and Social Sciences, Solent University, UK;
  $^b$Research Centre for Musculoskeletal and Sports Medicine, Manchester Metropolitan University, Manchester, UK;
  $^c$Health Sciences Department, CUNY Lehman College, Bronx, New York, USA;
  $^d$Evolution & Ecology Research Centre and School of Biological, Earth and Environmental Sciences, University of New South Wales, Sydney, New South Wales, Australia
output: 
  bookdown::pdf_document2:
    extra_dependencies: ["flafter"]
bibliography: references.bib
csl: apa.csl
toc: false
header-includes:
   - \usepackage{caption}
   - \captionsetup[figure]{font=small}
---

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align="center")
options(knitr.kable.NA = '')
```
```{r, message=FALSE, warning=FALSE, echo=FALSE}
# Load relevant packages
library(here)
library(tidyverse)
library(kableExtra)
library(patchwork)
library(ggtext)
```
```{r, message=FALSE,warning=FALSE,echo=FALSE}
# load the raw data to the environment
Data <- read.csv(here::here("data","Polito et al. RT Extracted Data.csv"), na.strings=c(""," ","NA"))

# load the summary dataframes to the environment
load(here("models/SMD_RR_SDir_logVR"))
load(here("models/mods_SMD_logCVR"))

```
# Introduction
Although the quantitative synthesis of results across studies has existed since the 17th century [@plackettStudiesHistoryProbability1958], the modern-day term "meta-analysis" was coined by Gene Glass [-@glassPrimarySecondaryMetaAnalysis1976]. Since that time, the use of meta-analysis as a tool for the synthesis of research in sport and exercise science has increased considerably [@haggerMetaanalysis2022], with resistance training (RT) accounting for a considerable proportion of this growth (figure \@ref(fig:trend-plot)). Accordingly, throughout the paper we use RT studies as a hopefully familiar example for sport and exercise science researchers. However, to begin we provide a conceptual overview of meta-analyses and effect sizes as typically used within sport and exercise science.

```{r trend-plot, echo=FALSE,fig.height=4, fig.width=6, fig.cap="Trends in meta-analyses published in sport and exercise science since 1976."}

load(here("plots/trends_plot"))
trends_plot

```

There are two popular models[^1] used for meta-analysis: the fixed-effect model and the random-effects model [@borensteinBasicIntroductionFixedeffect2010]. The fixed effect model assumes that there is one true effect size[^2] that each study included in the meta-analysis has estimated and that any differences in the estimates between individual studies are due to only sampling error. This essentially means that there is a single common effect which is fixed across studies and each study takes samples of individuals from the population to estimate this effect. We can express this model in the following formula:

[^1]: To clarify language here for those unfamiliar, the term and concept *model* is used commonly in statistics. A statistical model essentially is specification of what we think the data generating process might be for a given situation. In the context of meta-analyses the data are usually the individual effects that we have extracted from studies i.e., the results of each study. The model, in mathematical formulae, is intended to approximate the processes that we assume led to the generation of the data.

[^2]: *Effect size* is an agnostic term used for a family of statistics which communicate the strength of a given 'effect' resulting from research. This includes descriptive statistics ranging from mean raw values to correlation coeffients and everything in between [@caldwellCaseDefaultEffect2020] including, as we shall see, statistics describing variation.

\begin{equation}
\hat\theta_{i}=\theta+m_{i} 
(\#eq:fixed-model-eq)
\end{equation}

where $\hat\theta_{i}$ is the $i\textrm{th}$ effect size ($i = 1,2,\cdots,N_{i}$; where $N_{i}$ is the number of studies and thus effect sizes), $\theta$ is the intercept or overall mean (i.e., the fixed-effect), and $m_{i}$ is the sampling error for each effect size normally distributed with $\sigma^2_{m_{i}}$. Studies with smaller standard errors of their effect estimates have smaller sampling errors and so these higher precision estimates are given greater weight in the model. The weighting given to each study is calculated as:

\begin{equation}
w_{i}=\frac{1}{s^2_{i}}
(\#eq:fixed-weights-eq)
\end{equation}

Where $s_{i}$ is the standard deviation for the effect estimate and thus $s^2_{i}$ is the variance. It is referred to as inverse-variance weighting. Then, the overall weighted mean effect estimate from the model is then calculated as:

\begin{equation}
\hat\theta=\frac{\sum^I_{i=1}\hat\theta_{i}w_{i}}{\sum^I_{i=1}w_{i}}
(\#eq:fixed-average-eq)
\end{equation}

Contrastingly, the random effects model does not make the assumption that there is only one fixed-effect. Instead it allows for the true effects that each study estimates to differ. Each study may share a common underlying effect size but, due to differences between studies in factors such as population characteristics, the manner in which outcomes are operationalised, or subtle differences in intervention and context to name a few, it is possible that the actual effect being estimated by each study differs. The assumption of the random-effects model then is that the studies included estimate effects that come from a larger population of effects determined by the inclusion criteria for studies included. So, the model assumes that the studies included reflect a random sample of all possible permutations of study and the effects they estimate from this population distribution of studies and effects. Hence, in the fixed-effect model there is one effect and it is assumed to be *fixed* across studies, whereas in the random-effects model there are many and we examine an assumed *random* sample of them. We can express this model in the following formula:

\begin{equation}
\hat\theta_{i}=\theta_{i}+\tau_{i}+m_{i}
(\#eq:random-model-eq)
\end{equation}

where $\hat\theta_{i}$ is the $i\textrm{th}$ effect size ($i = 1,2,\cdots,N_{i}$; where $N_{i}$ is the number of studies and thus effect sizes), $\theta_i$ is the intercept or overall mean of the effects across each study, $\tau_{i}$ is the deviation from $\theta_{i}$ for the $i\textrm{th}$ study (i.e., the random-effects), and $m_{i}$ is the sampling error for each effect size normally distributed with $\sigma^2_{m_{i}}$. Essentially this model assumes that each individual study estimates an effect and there will be some sampling error in estimating it. But, the effects each study estimates comes from an overall distribution of true effects with a mean value (i.e., $\theta_{i}$). The weighting in any model employing random effects such as this requires a different approach as the variance of the distribution of the effect sizes the model assumes, known as $\tau^2$, must also be estimated[^3]. This essentially describes the heterogeneity between the effects included. Once this has been estimated we can calculate a weight that is adjusted for the random effects for each effect as:

[^3]: This estimation can be done using a variety of methods and is an area of ongoing investigation as to how different methods perform. This is beyond the scope of this paper to discuss. We note however that the models we present all utilise Restricted Maximum Likelihood estimation.

\begin{equation}
w^*_{i}=\frac{1}{s^2_{i}+\tau^2}
(\#eq:random-weights-eq)
\end{equation}

Then the average of the distribution of effects can be calculated as per equation \@ref(eq:fixed-average-eq) substituting $w_{i}$ for $w^*_{i}$.

Although historically fixed effect models were commonplace in the field of sport and exercise science [@haggerMetaanalysisSportExercise2006] nowadays the random effects model is more often employed [@haggerMetaanalysis2022]. This is likely due to the fact that direct (i.e., exact) replication of studies is rare[^4] and instead studies often explore similar effects across varying moderating factors such as those noted above.

[^4]: Hence current efforts to conduct direct replications (see https://ssreplicationcentre.com/).

As with many other fields [@nakagawaMetaanalysisVariationEcological2015; @usuiMetaanalysisVariationSuggests2021; @millsDetectingHeterogeneityIntervention2021] likely the most common aim in meta-analysis in sport and exercise science, and indeed primary empirical research too, is to estimate the *effect* of an independent variable upon some dependent outcome variable. The dependent variable is often the mean of a measurement and the independent variable is often a categorical grouping; for example, the comparison of an intervention group(s) and a control group, the comparison of intervention groups between one another, or comparison between non-manipulated categories such as biological sex. Indeed, a recent umbrella review [@bernardez-vazquezResistanceTrainingVariables2022] of meta-analyses in RT identified 14 studies examining the manipulation of RT intervention variables (i.e., the comparison of one intervention to another whereby a variable in the intervention was manipulated) on hypertrophy outcomes, all of which focused on the comparison of mean changes between different intervention groups. 
 
Often, due to the varying operationalisations used for broad outcome concepts[^5], an effect size is used which is "standardised" across studies to allow for their synthesis. Most commonly, a magnitude based[^6] effect size statistic [@caldwellCaseDefaultEffect2020], the standardised mean difference (SMD), is used to compare means between groups or conditions. This statistic is usually a version of Cohen's *d* [@cohenStatisticalPowerAnalysis1988], or its bias-corrected[^7] metric referred to as Hedges' *g* [@hedgesStatisticalMethodsMetaAnalysis2014; @borensteinIntroductionMetaAnalysis2021; @nakagawaEffectSizeConfidence2007][^8]. The SMD, and its sampling variance, $s^2_{SMD}$ are given by:

[^5]: For example, *strength* might be examined in different studies using different operationalisations including one repetition maximum testing or maximum voluntary contractions. Or the same operationalisations may be employed but different exercises such as the squat or bench press.

[^6]: Though notably not all meta-analyses use *magnitude based* effect sizes. Indeed some explicitly use what Caldwell and Vigotsky [-@caldwellCaseDefaultEffect2020] term *signal-to-noise* effect sizes (e.g., @heidelMachinesFreeWeight2022).

[^7]: For those unfamiliar with the terminology, an estimator for a statistic is unbiased if it produces parameter estimates that are on average correct. Thus a bias corrected statistic is one which would be biased without the correction applied, but otherwise has been shown to be unbiased.

[^8]: We will refer to both merely as the SMD throughout the manuscript for simplicity and note that throughout when reporting a 'SMD' we are reporting the bias-corrected version. We also note that another magnitude based effect size, Glass' $\Delta$, is commonly recommended as it is the simplest form of SMD though makes assumptions about the impact of the intervention having no effect on the denominator (i.e., variance; @caldwellCaseDefaultEffect2020).

\begin{equation}
SMD=\frac{\overline{x}_E - \overline{x}_C}{s_{pooled}}J
(\#eq:SMD-eq)
\end{equation}

\begin{equation}
J=1-\frac{3}{4(n_{C}+n_{E})-2)-1}
(\#eq:biasJ-eq)
\end{equation}

\begin{equation}
s_{pooled}=\sqrt{\frac{(n_{C}-1)s^2_{C}+(n_{E}-1)s^2_{E}}{n_{C}+n_{E}-2}}
(\#eq:s-pooled-eq)
\end{equation}

\begin{equation}
s^2_{SMD}=\frac{n_{C}+n_{E}}{n_{C}n_{E}}+\frac{SMD^2}{2(n_{E}+n_{C})}
(\#eq:SMDvar-eq)
\end{equation}

where $\overline{x}_C$ and $\overline{x}_E$ are the sample means of the control group (C) and experimental (E) or intervention group respectively, $s_C$ and $s_E$ are the standard deviations of the two groups, $n_C$ and $n_E$ are the sample sizes of the two groups, and $J$ is a bias correction for small sample sizes.

The natural logarithm of the ratio of two means ($\textrm{ln}RR$) is also another effect size statistic that can be used [@curtisMetaanalysisElevatedCO21998; @hedgesMetaAnalysisResponseRatios1999; @lajeunesseMetaanalysisResponseRatios2011; @lajeunesseBiasCorrectionLog2015]. The lnRR, and its sampling variance, $s^2_{\textrm{ln}RR}$ are given by:

\begin{equation}
\textrm{ln}RR=\textrm{ln}\frac{\overline{x}_E}{\overline{x}_C}
(\#eq:lnRR-eq)
\end{equation}

\begin{equation}
s^2_{\textrm{ln}RR}=\frac{s^2_{C}}{n_{C}\overline{x}^2_{C}}+\frac{s^4_{C}}{2n_{C}\overline{x}^2_{C}\overline{x}^4_{C}}+\frac{s^2_{E}}{n_{E}\overline{x}^2_{E}}+\frac{s^4_{E}}{2n_{E}\overline{x}^2_{E}\overline{x}^4_{E}}
(\#eq:lnRRvar-eq)
\end{equation}

Due to its calculation the SMD is affected not only by the difference in means of the two groups, but also by the standard deviations of both groups due to the standardisation of the effect size by $s_{pooled}$ in the denominator. In contrast, the $\textrm{ln}RR$ is uninfluenced by the standard deviations in either groups (see equation \@ref(eq:lnRR-eq)), which only affects the sampling variance (see equation \@ref(eq:lnRRvar-eq)). Despite this, the use of effect sizes like the $\textrm{ln}RR$ has been limited in previous meta-analyses in sport and exercise science [@debQuantifyingEffectsAcute2018; @nuzzoEccentricConcentricStrength2023] and to our knowledge only a couple of meta-analyses of RT interventions has used this kind of effect size [@swintonInterpretingMagnitudeChange2022; @wolfPartialVsFull2023].

Although researchers in sport and exercise science, among other fields, have focused on estimating the average effects of interventions using randomised trial designs for both primary research and synthesis through meta-analysis, responses to certain interventions may vary on a subgroup or even individual basis. The increased interest in *precision* or *personalised* approaches to exercise prescription has resulted in a number of opinion and methodological review articles discussing statistical approaches to understanding interindividual response heterogeneity to exercise interventions [@heckstedenIndividualResponseExercise2015; @atkinsonTrueFalseInterindividual2015; @atkinsonIssuesDeterminationResponders2019; @rossPrecisionExerciseMedicine2019; @swintonStatisticalFrameworkInterpret2018; @hopkinsIndividualResponsesMade2015; @kelleyPrecisionExerciseMedicine2022; @hrubeniukDirectionsExerciseTreatment2022; @pickeringNonRespondersExerciseExistand2019]. However, despite the availability of approaches to compare variances between groups, in sport and exercise science this is rarely explored in primary research [@bonafigliaInterindividualDifferencesTrainability2022]. Moreover, although there has been increased interest in recent years, few meta-analyses in sport and exercise include both comparisons of means and variances or explicitly aim to investigate the latter [@kelleyAreThereInterindividual2022; @kelleyAreThereInterIndividual2020; @estevesIndividualParticipantData2021; @bonafigliaInterindividualDifferencesTrainability2022; @steeleSlowSteadyHard2021; @fisherRoleSupervisionResistance2022]. Examination of interindividual heterogeneity in response to interventions presents considerable value to researchers and practitioners in sport and exercise science; interventions with low interindividual variation are likely to be widely generalisable, whilst an intervention with high interindividual variation is likely to have effects that are either subgroup or individual specific. The former kind of intervention might be widely applicable across individuals, whilst the latter kind of intervention requires specific research, typically with large samples [@heckstedenIndividualResponseExercise2015], to tease out subgroup- or participant-by-intervention interactions to facilitate successful practical application.

Comparison of heterogeneity in responses, such as post-scores or change scores to interventions, are not the only possible use of statistical methods for comparing variances. For example, in other fields such as ecology there have been calls to shift focus of analysis onto the exploration of dispersion of traits between groups in non-experimental or intervention designs [@nakagawaMeanStrikesBack2012]. Some recent examples from sport and exercise science, and RT in particular, include primary research exploring between-participant acute response variation for the purposes of identifying methods[^9] to reduce RT stimulus heterogeneity, [@exnerDoesPerformingResistance2022] as well as a meta-analysis exploring between-participant heterogeneity of accuracy in predicting proximity to task failure during RT [@halperinAccuracyPredictingRepetitions2022] and in the number of repetitions that can be performed at different percentages of one repetition maximum [@nuzzoMaximalNumberRepetitions2023].

[^9]: Exploration of methodological approaches and their impact on heterogeneity have also been explored in preclinical research [@usuiMetaanalysisVariationSuggests2021].

Given the value of embracing and exploring variation alongside mean effects in sport and exercise science, yet the lack of application in research synthesis by way of primary research or meta-analysis, we present and discuss effect size approaches and models for meta-analysis of variation. Indeed, meta-analysis presents a very valuable method for exploring variation in a field such as sport and exercise science due to the typically small samples in primary studies. Such small samples have even lower statistical power to detect differences in variation as compared to means [@yangLowStatisticalPower2022].

Although ultimately we will recommend the thoughtful consideration of assumptions for certain approaches regarding what effect size statistics and models to employ in examining variation through use of meta-analysis, we provide examples throughout using all approaches described in order to aid the reader in understanding their strengths and weaknesses. We hope this will make clear why we offer such recommendations in our discussion and conclusion. As we have up to this point, we will also make an effort to both present the mathematical formulations of the models described, as well as to provide an explanation of them in plain English.

# Effect size statistics for meta-analytic comparisons of variation
Until recent years there has been a dearth of effect size statistics available for the examination of variation in a meta-analytic framework. However, several have been proposed that we now describe: the standard deviation for individual responses ($SD_{ir}$; @hopkinsIndividualResponsesMade2015; @atkinsonTrueFalseInterindividual2015; @atkinsonIssuesDeterminationResponders2019), the log ratio of standard deviations ($\textrm{ln}VR$; termed the "variability ratio"; @hedgesSexDifferencesMental1995), and the log ratio of coefficient of variation ($\textrm{ln}CVR$; termed the "coefficient of variation ratio"; @nakagawaMetaanalysisVariationEcological2015; @seniorRevisitingExpandingMetaanalysis2020). We present the independent groups versions due to use of randomised controlled trials in our examples below, but note that dependent versions (i.e., for comparing related samples) also exist for $\textrm{ln}VR$ and $\textrm{ln}CVR$ [@seniorRevisitingExpandingMetaanalysis2020].

## Standard deviation for individual responses ($SD_{ir}$)
In the context of *precision* or *personalised* approaches to exercise prescription the $SD_{ir}$ has been proposed as an approach to determine the extent to which individual responses manifest by comparison of variation between two groups; control and intervention [@hopkinsIndividualResponsesMade2015; @atkinsonTrueFalseInterindividual2015; @atkinsonIssuesDeterminationResponders2019]. The standard deviation of change scores (post-intervention scores minus pre-intervention scores) within the intervention group reflects the gross combination of a number of sources of variation including: participant-by-intervention interactions (i.e., actual individual responsiveness or 'trainability'), within-participant variability in intervention response (i.e., variability in response to the same intervention administered to the same participant), and random error (i.e., from pre and post measurements; @heckstedenIndividualResponseExercise2015). The standard deviation of change scores from the control group (assuming it is a non-intervention control group and not something like a 'usual-care' group) by contrast is assumed to only reflect random error[^10] [@heckstedenIndividualResponseExercise2015]. As such, the difference in these standard deviations can be used to determine the extent to which additional variation has been introduced by the intervention and that might reflect individual responses. Whilst the $SD_{ir}$ has been proposed and used primarily in the context of individual response variation to interventions, it should be noted that this kind of absolute comparison of variance between groups or conditions is not limited to such applications.

[^10]: Though notably, in the case of health behaviour studies it may be the case that if someone volunteers for a study it could conceivably motivate them to alter various habits even when they are assigned to a control group thus influencing change scores.

The $SD_{ir}$, and its sampling variance, $s^2_{SD_{ir}}$ are given by:

\begin{equation}
SD_{ir}=\sqrt{s^2_{E} - s^2_{C}}
(\#eq:SDir-eq)
\end{equation}

\begin{equation}
s^2_{SD_{ir}}=2\biggl(\frac{s^4_{E}}{n_{E}-1}+\frac{s^4_{C}}{n_{C}-1}\biggl)
(\#eq:sdIRvar-eq)
\end{equation}

Thus, the $SD_{ir}$ reflects a comparison of the absolute variance in change scores between control and intervention groups. However, a potential concern with the $SD_{ir}$ is its potential to violate assumptions of normality, which is not the case for other effect size statistics such as $\textrm{ln}VR$ and $\textrm{ln}CVR$.

## Log ratio of standard deviations ($\textrm{ln}VR$)
A similar effect size statistic for the comparison of absolute variance between groups, and one that has had wide applications in more than just intervention response variability within fields such as ecology and evolution, is the $\textrm{ln}VR$ [@hedgesSexDifferencesMental1995; @nakagawaMetaanalysisVariationEcological2015; @seniorRevisitingExpandingMetaanalysis2020]. An unbiased estimator of the natural logarithm of a population standard deviation ($\textrm{ln}\sigma$), and its sampling variance, $s^2_{\textrm{ln}\sigma}$ is given by: 

\begin{equation}
\textrm{ln}\hat\sigma=\textrm{ln}s+\frac{1}{2(n-1)}
(\#eq:lnsigma-eq)
\end{equation}

\begin{equation}
s^2_{\textrm{ln}\hat\sigma}=\frac{1}{2(n-1)}
(\#eq:lnsigmavar-eq)
\end{equation}

where $\textrm{ln}\hat\sigma$ is an estimate of $\textrm{ln}\sigma$, and it is assumed with sufficiently large sample size and value of $\sigma$ that $\textrm{ln}\sigma$ is normally distributed with variance $s^2_{\textrm{ln}\sigma}$. Given equations \@ref(eq:lnsigma-eq) and \@ref(eq:lnsigmavar-eq), the logarithm of the ratio of standard deviations of two groups, such as a control and intervention, the $\textrm{ln}VR$, and its sampling variance, $s^2_{\textrm{ln}VR}$ is given by: 

\begin{equation}
\textrm{ln}VR=\textrm{ln}\biggl(\frac{s_{E}}{s_{C}}\biggl)+\frac{1}{2(n_{E}-1)}-\frac{1}{2(n_{C}-1)}
(\#eq:lnVR-eq)
\end{equation}

\begin{equation}
s^2_{\textrm{ln}VR}=\frac{1}{2}\biggl(\frac{n_{C}}{(n_{C}-1)^2}+\frac{n_{E}}{(n_{E}-1)^2}\biggl)
(\#eq:lnVRvar-eq)
\end{equation}

However, due to both $SD_{ir}$ and $\textrm{ln}VR$ being comparisons of absolute variance, they may find limited applicability where the mean of one group is larger than the comparison group (e.g., when $\overline{x}_E$ is larger than $\overline{x}_C$). In this case, it is likely that the standard deviation will be larger in the group with the larger mean (e.g., $s_{E}$ is larger than $s_{C}$). This mean-variance relationship is common for many variables and datasets[^11] and to highlight this we provide examples below. They also assume constant measurement error over the range of values for the mean, which can impact their utility for examining response variation [@tenanCommentMethodStop2020].

[^11]: For one clear example, see figure 1A in Vigostky et al. [-@vigotskyImprobableDataPatterns2020] who show that the mean and standard deviation for baseline strength values typically scale with one another across most studies.

## Log ratio of coefficient of variation ($\textrm{ln}CVR$)
The coefficient of variation is the ratio of the standard deviation to the mean; therefore, comparison of the coefficient of variation between groups will identify whether standard deviations differ more, or less, than would be predicted by their difference in means where a mean-variance relationship is present. In essence, the coefficient of variation is a means of standardising the standard deviation against the mean such that the relative variation in an effect is expressed. The natural logarithm of the ratio between the coefficients of variation from two groups, the $\textrm{ln}CVR$ is thus a more generally applicable effect size statistic for examining variability between groups. Considering equations \@ref(eq:lnRR-eq) and \@ref(eq:lnVR-eq), the $\textrm{ln}CVR$ is given by:

\begin{equation}
\textrm{ln}CVR=\textrm{ln}\biggl(\frac{CV_{E}}{CV_{C}}\biggl)+\frac{1}{2(n_{E}-1)}-\frac{1}{2(n_{C}-1)}
(\#eq:lnCVR-eq)
\end{equation}

where $CV_{E}$ and $CV_{C}$ are ${s_{E}}/{\overline{x}_{E}}$ and ${s_{C}}/{\overline{x}_{C}}$ respectively. Senior et al. [-@seniorRevisitingExpandingMetaanalysis2020] derived the sampling variance, $s^2_{\textrm{ln}CVR}$, as:

\begin{equation}
\begin{split}
s^2_{\textrm{ln}CVR}=\frac{s_{C}^2}{n_{C}\overline{x}_{C}^2}+\frac{s_{C}^4}{2n_{C}^2\overline{x}_{C}^4}+\frac{n_{C}}{(n_{C}-1)^2} \\
+\frac{s_{E}^2}{n_{E}\overline{x}_{E}^2}+\frac{s_{E}^4}{2n_{E}^2\overline{x}_{E}^4}+\frac{n_{E}}{(n_{E}-1)^2}
\end{split}
(\#eq:lnCVRvar-eq)
\end{equation}

# Examples using resistance training studies
As noted, to facilitate understanding for those new to examination of variation, we provide primary examples of the approaches presented using data from RT studies included in a recent meta-analysis published in the *Journal of Sport Sciences* [@politoModeratorsStrengthGains2021]. Here we have used their list of included studies and re-extracted data from 111 of these[^12]. All analysis examples were performed in R (version 4.2.1, "Funny-Looking Kid", The R Foundation for Statistical Computing, 2022) using the **metafor** package [@viechtbauerConductingMetaAnalysesMetafor2010]. The extracted dataset, analysis scripts, models, data summaries, and supplementary materials are available on the Open Science Framework (https://osf.io/2h9ma/) or the GitHub repository (https://github.com/jamessteeleii/Meta-Analysis-of-Variation-in-Resistance-Training.git).

[^12]: The authors of the meta-analysis did not make their extracted data openly available, nor did they respond to our request for the extracted data. Further, their original analysis included 119 studies however we were unable to extract data for our analyses from 8 of these for a variety of reasons (e.g., only percentage change data was reported, no standard deviations for control groups reported).

```{r sample-size-tab, message=FALSE, warning=FALSE, echo=FALSE}

load(here("models/sample_sizes"))

knitr::kable(
  sample_sizes,
  caption = 'Sample sizes for resistance training and non training control groups for dataset.'
  ) %>%
  pack_rows("RT", 1, 4) %>%
  pack_rows("CON", 5, 8) %>%
  footnote(general = c("RT = resistance training", "CON = non-training control")
           ) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = FALSE) 

```

Polito et al. [-@politoModeratorsStrengthGains2021] conducted a systematic review and meta-analysis of randomised trials that included a RT intervention group(s) and a non-training control comparison group. Their analysis focused upon the SMD between the RT intervention group(s) and the control group from the studies included, with both overall effect estimate and moderator analyses (i.e., meta-regressions[^13]) performed. Given that Polito et al. [-@politoModeratorsStrengthGains2021] included only studies with a non-training control group, their study selection offers a unique context to examine variation of interindividual responses specifically by comparing the variances in change scores between the RT intervention groups(s) and control group. Table \@ref(tab:sample-size-tab) shows the total sample size, along with the median and range by group, across the included studies. Indeed, this highlights the typically small samples used in sport and exercise science, and thus low power to detect difference in both means and variances in in individual studies [@yangLowStatisticalPower2022], emphasising the value of meta-analysis to explore variation. Table \@ref(tab:summary-characteristics-tab) shows the study and participant characteristics.

[^13]: Regression analyses are likely familiar to most readers where in the simplest form they try to predict the value of some dependent variable from some independent variable(s). This can be extended to meta-analytic synthesis where the independent variables reflect characteristics associated with the effects included. For example, they may reflect characteristics of the sample in the study for which the effect was extracted such as age or sex, or they might reflect characteristics of the intervention received such as the dose or frequency of exposure.

```{r summary-characteristics-tab, message=FALSE, warning=FALSE, echo=FALSE}

load(here("models/summary_table"))

knitr::kable(
  summary_table,
  caption = 'Summary of study and participant characteristics.',
  align = c("l","c") 
  ) %>%
  pack_rows("Training Status",6, 7, bold = FALSE) %>%
  pack_rows("Sample Type", 8, 9, bold = FALSE) %>%
  pack_rows("RT Intervention Only?", 10, 11, bold = FALSE) %>%
  pack_rows("Task Failure?", 18, 19, bold = FALSE) %>%
  footnote(general = c("RT = resistance training; ", "Continuous variables are median (IQR); ", "Categorical variables are count (%); ", "Not all studies reported full descriptive data (see dataset; https://osf.io/kg2z4)")
           ) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = FALSE) %>%
    kable_styling()

```

## Detecting the presence of interindividual response variation to resistance training intervention with absolute variance statistics
First we conducted a traditional SMD and $\textrm{ln}RR$ based effect size[^14] meta-analysis to explore the effects of RT interventions compared to controls for strength and hypertrophy (i.e., muscle mass/size) outcomes[^15]. Polito et al. [-@politoModeratorsStrengthGains2021] originally used a normal random-effects meta-analysis as described in the introduction. However, the data we extracted were hierarchical in nature. Thus, as opposed to there being only the assumption that studies are a random effect, due to there being multiple outcomes measured within each arm in the studies (i.e., intervention group(s) and control group, within each study), and that in some studies there were multiple interventions examined, there is the additional assumption that must be included that both the intervention groups and effects also come from overarching distributions. Thus a multilevel mixed-effects meta-analysis model [@vandennoortgateThreelevelMetaanalysisDependent2013] with cluster-robust variance estimation [@hedgesRobustVarianceEstimation2010] was used with random intercepts for study, arm[^16], and effect. This model then includes additional $\tau^2$ terms for each of the levels and assigns weights appropriately given this and the clustering of effects within arms within studies. We can therefore describe the overall model as:

[^14]: It is worth noting that in the sport and exercise sciences, similarly to other fields that examine the effects of experimental intervention, the most common study design for testing or estimating intervention effects is the randomised pretest-postest-control design (i.e., an intervention and control, or other intervention, group randomly allocated and measured pre- and post-exposure). We presented the SMD and $\textrm{ln}RR$ effect sizes in equations \@ref(eq:SMD-eq) and \@ref(eq:lnRR-eq) merely for simplicity in the introduction, but note that extension of these for such 2x2 (i.e., condition x time) study designs have been presented in detail elsewhere (see: Gurevitch et al., [-@gurevitchInteractionCompetitionPredation2000]; Morris et al., [-@morrisDirectInteractiveEffects2007]; Morris [-@morrisEstimatingEffectSizes2008]; Lajeunesse [-@lajeunesseMetaanalysisResponseRatios2011; -@lajeunesseBiasCorrectionLog2015]) and these are the effect sizes used in the meta-analyses referred to here.

[^15]: We also explored for signs of small study bias, including publication bias favouring the finding of intervention effects, for the SMDs given that the relative lack of awareness for variance based effect sizes in the field implies that they might have more influence over such biases. There did not appear to be any obvious small study bias in the dataset (see https://osf.io/stqr3).  

[^16]: We use the term *arm* to refer to an intervention group-control group contrast to accommodate studies including multiple intervention groups. This is so as to not confuse the reader with the use of *group* to designate either the RT intervention group(s) or control group separately. Thus, in the instances of models using effect sizes relating to comparisons between an intervention group and control group (i.e., SMD, $\textrm{ln}RR$, $SD_{ir}$, $\textrm{ln}VR$, and $\textrm{ln}CVR$) we calculate comparisons *between* each intervention group (i.e., arm) and the control group. Thus, where a study had for example two RT interventions and a control, two separate arms would be coded (RT intervention 1 compared to control, and RT intervention 2 compared to control). Data was coded such that study and arm had explicit nesting.

\begin{equation}
\hat\theta_{ijk}=(\theta+\tau_{(1)i}+\tau_{(2)j}+\tau_{(3)k})+m_{ijk}
(\#eq:ml-model-eq)
\end{equation}

where $\hat\theta_{ijk}$ is the $k\textrm{th}$ effect size, here the SMD or $\textrm{ln}RR$, from the $j\textrm{th}$ arm ($j = 1,2,\cdots,N_{j}$; where $N_{j}$ is the number of arms) in the $i\textrm{th}$ study ($i = 1,2,\cdots,N_{i}$; where $N_{i}$ is the number of studies), $\theta$ is the intercept or overall mean of the effects, $\tau_{i}$ is the deviation from $\theta$ for the $i\textrm{th}$ study,  $\tau_{j}$ is the deviation for the $j\textrm{th}$ arm, $\tau_{k}$ is the deviation for the $k\textrm{th}$ effect, and $m_{ijk}$ is the sampling error for each effect size normally distributed with $\sigma^2_{\theta_{ijk}}$. This model is referred to as 'mixed' effects because of the presence of both fixed ($\theta$), and random ($\tau_{(1)i},\tau_{(2)j},\tau_{(3)k}$) effects[^17]. The main term in the model we are interested in is $\theta$ which is our estimate for the overall weighted average effect (i.e., $\hat\theta$).

[^17]: Technically then the *random* effects model presented earlier is also a *mixed* effects model. It is traditionally referred to as the random-effects model though.

We then fit the same model for the $SD_{ir}$ and $\textrm{ln}VR$ effect sizes for change scores (i.e., post-intervention minus pre-intervention scores) in order to explore how absolute variance in responses differed between RT interventions and controls. A positive SMD or $\textrm{ln}RR$ would indicate that RT interventions produced greater improvements in outcomes compared to controls, whilst a positive $SD_{ir}$ and $\textrm{ln}VR$ would indicate that the introduction of the RT intervention increased variation in responses (i.e., change scores) compared to controls (i.e., suggests the presence of interindividual response variation). 

```{r forest-SMD-plot, message=FALSE, warning=FALSE, echo=FALSE, fig.height=10, fig.width=10, fig.cap="Caterpillar plots of SMD effect sizes for strength (A) and hypertrophy (B) outcomes."}

# "Caterpillar plots of SMD effect sizes for strength and hypertrophy outcomes. Note, each estimate and 95% confidence interval is a single observed effect size within each arm within each study, the diamond reflects the model overall estimate and 95% confidence interval, and the horizontal line with vertical bars at the end reflect the 95% prediction interval"

load(here("plots/forest_SMD_strength"))
load(here("plots/forest_SMD_hypertrophy"))
library(patchwork)
(forest_SMD_strength / forest_SMD_hypertrophy) +
  plot_annotation(tag_levels = "A")

```

The pattern of results from our models examining SMDs (figure \@ref(fig:forest-SMD-plot)) were similar to those reported by Polito et al. [-@politoModeratorsStrengthGains2021], albeit with slightly lower estimates for both outcome types; possibly due to our use of a multilevel mixed-effects meta-analysis model that allowed for each individual effect size to be more appropriately weighted (the relative amount of heterogeneity between effects for each level is presented as the $I^2$ statistic). As might be expected, in comparison to non-training controls the RT interventions produced increases in both strength (SMD = `r SMD_RR_SDir_logVR$Estimate[1]` [95%CI: `r SMD_RR_SDir_logVR$Lower[1]` to `r SMD_RR_SDir_logVR$Upper[1]`]; $I^2_{study}$ = `r SMD_RR_SDir_logVR$I.2.study[1]`%, $I^2_{arm}$ = `r SMD_RR_SDir_logVR$I.2.arm[1]`%, $I^2_{effect}$ = `r SMD_RR_SDir_logVR$I.2.effect[1]`%) and hypertrophy outcomes (SMD = `r SMD_RR_SDir_logVR$Estimate[5]` [95%CI: `r SMD_RR_SDir_logVR$Lower[5]` to `r SMD_RR_SDir_logVR$Upper[5]`]; $I^2_{study}$ = `r SMD_RR_SDir_logVR$I.2.study[5]`%, $I^2_{arm}$ = `r SMD_RR_SDir_logVR$I.2.arm[5]`%, $I^2_{effect}$ = `r SMD_RR_SDir_logVR$I.2.effect[5]`%). Confidence intervals on the overall effects were precise for both outcomes though prediction intervals, indicating the range over which we might expect future estimates of effects to fall based on this evidence, for SMD estimates (see figure \@ref(fig:forest-SMD-plot)) were fairly wide and relative heterogeneity was fairly high mostly as a result of between-study variance (i.e., effects were more similar *within* studies and arms than *between* them).

```{r forest-RR-plot, message=FALSE, warning=FALSE, echo=FALSE, fig.height=10, fig.width=10, fig.cap="Caterpillar plots of exponentiated RR effect sizes for strength (A) and hypertrophy (B) outcomes."}

# "Caterpillar plots of exponentiated RR effect sizes for strength and hypertrophy outcomes. Note, each estimate and 95% confidence interval is a single observed effect size within each arm within each study, the diamond reflects the model overall estimate and 95% confidence interval, and the horizontal line with vertical bars at the end reflect the 95% prediction interval"

load(here("plots/forest_RR_strength"))
load(here("plots/forest_RR_hypertrophy"))
library(patchwork)
(forest_RR_strength / forest_RR_hypertrophy) +
  plot_annotation(tag_levels = "A")

```

For the $\textrm{ln}RR$ results we exponentiated them and converted to percentages to be more interpretable. These were similar, with greater proportional increases in strength compared with hypertrophy (figure \@ref(fig:forest-RR-plot)). Increases were seen for both strength ($\textrm{exp}RR$ = `r SMD_RR_SDir_logVR$Estimate[2]` [95%CI: `r SMD_RR_SDir_logVR$Lower[2]` to `r SMD_RR_SDir_logVR$Upper[2]`]; $I^2_{study}$ = `r SMD_RR_SDir_logVR$I.2.study[2]`%, $I^2_{arm}$ = `r SMD_RR_SDir_logVR$I.2.arm[2]`%, $I^2_{effect}$ = `r SMD_RR_SDir_logVR$I.2.effect[2]`%) and hypertrophy ($\textrm{exp}RR$ = `r SMD_RR_SDir_logVR$Estimate[6]` [95%CI: `r SMD_RR_SDir_logVR$Lower[6]` to `r SMD_RR_SDir_logVR$Upper[6]`]; $I^2_{study}$ = `r SMD_RR_SDir_logVR$I.2.study[6]`%, $I^2_{arm}$ = `r SMD_RR_SDir_logVR$I.2.arm[6]`%, $I^2_{effect}$ = `r SMD_RR_SDir_logVR$I.2.effect[6]`%). Confidence intervals were again precise for both outcomes, and whilst relative heterogeneity was lower compared to SMD models prediction intervals were still quite wide.

In addition to the SMD and $\textrm{ln}RR$ results, both the $SD_{ir}$ (figure \@ref(fig:forest-SDir-plot)) and $\textrm{ln}VR$ (figure \@ref(fig:forest-logVR-plot)) were also positive for both strength ($SD_{ir}$ = `r SMD_RR_SDir_logVR$Estimate[3]` [95%CI: `r SMD_RR_SDir_logVR$Lower[3]` to `r SMD_RR_SDir_logVR$Upper[3]`]; $I^2_{study}$ = `r SMD_RR_SDir_logVR$I.2.study[3]`%, $I^2_{arm}$ = `r SMD_RR_SDir_logVR$I.2.arm[3]`%, $I^2_{effect}$ = `r SMD_RR_SDir_logVR$I.2.effect[3]`%; $\textrm{ln}VR$ = `r SMD_RR_SDir_logVR$Estimate[4]` [95%CI: `r SMD_RR_SDir_logVR$Lower[4]` to `r SMD_RR_SDir_logVR$Upper[4]`]; $I^2_{study}$ = `r SMD_RR_SDir_logVR$I.2.study[4]`%, $I^2_{arm}$ = `r SMD_RR_SDir_logVR$I.2.arm[4]`%, $I^2_{effect}$ = `r SMD_RR_SDir_logVR$I.2.effect[4]`%) and hypertrophy outcomes ($SD_{ir}$ = `r SMD_RR_SDir_logVR$Estimate[7]` [95%CI: `r SMD_RR_SDir_logVR$Lower[7]` to `r SMD_RR_SDir_logVR$Upper[7]`]; $I^2_{study}$ = `r SMD_RR_SDir_logVR$I.2.study[7]`%, $I^2_{arm}$ = `r SMD_RR_SDir_logVR$I.2.arm[7]`%, $I^2_{effect}$ = `r SMD_RR_SDir_logVR$I.2.effect[7]`%; $\textrm{ln}VR$ = `r SMD_RR_SDir_logVR$Estimate[8]` [95%CI: `r SMD_RR_SDir_logVR$Lower[8]` to `r SMD_RR_SDir_logVR$Upper[8]`]; $I^2_{study}$ = `r SMD_RR_SDir_logVR$I.2.study[8]`%, $I^2_{arm}$ = `r SMD_RR_SDir_logVR$I.2.arm[8]`%, $I^2_{effect}$ = `r SMD_RR_SDir_logVR$I.2.effect[8]`%) indicating that exposure to the RT interventions may have introduced additional variance over and above random error, potentially suggesting the presence of interindividual response variation. Although, heterogeneity across the models and levels (study, arm, effect) were again relatively large and quite varied. 

This additional variance might support previous perspectives [@carpinelliInterindividualHeterogeneityAdaptations2017] that the considerable variation in responses to RT interventions typically observed are due to 'true' interindividual response variation over and above the random error that occurs from pre- and post-intervention measurements (i.e., the variation is *detectable* independent of the random error). However, as noted, both the $SD_{ir}$ and $\textrm{ln}VR$ assume constant variance over values of the mean (i.e., that the variance is similar whether mean values are low or high). As we have seen from the SMD and $\textrm{ln}RR$ models, RT interventions increase mean scores. Thus, if there is a mean-variance relationship in the data, an increase in the mean alone may be fully responsible for any apparent increase in variation. As such, we cannot rely solely on absolute comparisons of variance such as the $SD_{ir}$ and $\textrm{ln}VR$ to determine whether interindividual response variation is actually present. The $\textrm{ln}CVR$ can be used to overcome this issue, and below we re-analyse this dataset using this effect size statistic. First though, we present data demonstrating the ubiquity of the mean-variance relationship in typical RT study outcome measures to emphasise the need to consider this assumption, and introduce a modelling approach that can also be used to overcome some possible limitations with the $\textrm{ln}CVR$ and provide flexibility to accomodate wider applications.

```{r forest-SDir-plot, message=FALSE, warning=FALSE, echo=FALSE, fig.height=10, fig.width=10, fig.cap="Caterpillar plots of SDir effect sizes for strength (A) and hypertrophy (B) outcomes."}

# "Caterpillar plots of SDir effect sizes for strength and hypertrophy outcomes. Note, each estimate and 95% confidence interval is a single observed effect size within each arm within each study, the diamond reflects the model overall estimate and 95% confidence interval, and the horizontal line with vertical bars at the end reflect the 95% prediction interval"

load(here("plots/forest_SDir_strength"))
load(here("plots/forest_SDir_hypertrophy"))
library(patchwork)
(forest_SDir_strength / forest_SDir_hypertrophy) +
  plot_annotation(tag_levels = "A")

```
```{r forest-logVR-plot, message=FALSE, warning=FALSE, echo=FALSE, fig.height=10, fig.width=10, fig.cap="Caterpillar plots of ln$VR$ effect sizes for strength (A) and hypertrophy (B) outcomes."}

# "Caterpillar plots of $\textrm{ln}RR$ effect sizes for strength and hypertrophy outcomes. Note, each estimate and 95% confidence interval is a single observed effect size within each arm within each study, the diamond reflects the model overall estimate and 95% confidence interval, and the horizontal line with vertical bars at the end reflect the 95% prediction interval"

load(here("plots/forest_logVR_strength"))
load(here("plots/forest_logVR_hypertrophy"))
library(patchwork)
(forest_logVR_strength / forest_logVR_hypertrophy) +
  plot_annotation(tag_levels = "A")

```

## Mean-variance relationships in muscular strength and hypertrophy
With meta-analytic models of variation we are not limited to solely exploring variation in responses to interventions (e.g., @halperinAccuracyPredictingRepetitions2022; @nuzzoMaximalNumberRepetitions2023). We can explore the relationships between variance in a number of outcomes and the impact of certain predictors on this in the form of meta-regression. For example, as noted, one possible predictor of variance is the mean itself. As such, we can model variance of each effect as the response itself with the mean of the effect as the predictor. The standard deviation is, however, bounded at zero and so in many cases it may not conform to assumptions of normality which are required for regression models (i.e., that the residuals, the difference between the estimated and actual data, are normally distributed). Therefore, we instead can use $\textrm{ln}\hat\sigma$, which is unbounded. In the following example we explore the mean-variance relationship in the pre-intervention scores for outcomes in the data set from Polito et al. [-@politoModeratorsStrengthGains2021].

```{r mean-variance-pre-plot, message=FALSE, warning=FALSE, echo=FALSE,fig.height=8, fig.width=10, fig.cap="Scatter plots of raw mean and standard deviation of pre-intervention scores for (A) strength outcomes and (B) hypertrophy outcomes, and of the log mean and log standard deviation of pre-intervention scores for (C) strength outcomes and (D) hypertrophy outcomes."}

load(here("plots/mean_variance_pre_plots"))

mean_variance_pre_plots

```

As can be seen in figure \@ref(fig:mean-variance-pre-plot)(A) and (C), there is considerable heteroskedasticity in the relationship between the raw mean ($\overline{x}$) and standard deviation ($s$). The variance in standard deviations increases with higher mean values. This is similar to what is known as Taylor's law in ecology, or the power law; in essence, an empirically derived relationship stating that the variance is a power function of the mean in many biological and physical systems [@taylorAggregationVarianceMean1961]. 

\begin{equation}
s^2=a\overline{x}^b
(\#eq:taylors-eq)
\end{equation}

where $a$ and $b$ are some constants. When this relationship holds, under most circumstances the standard deviation is not proportional to the mean. However, when the mean and standard deviation are transformed to the log scale this relationship becomes linear based upon the product and power logarithmic rules:

\begin{equation}
2\textrm{ln}s=\textrm{ln}a+b\textrm{ln}\overline{x}
(\#eq:taylorsln-eq)
\end{equation}

Figure \@ref(fig:mean-variance-pre-plot)(B) and (D) shows that the relationship between the mean and variance on the log scale better meets the assumption of normality. Given these the observations we have for $\textrm{ln}\hat\sigma$ and $\textrm{ln}\overline{x}$ come from multiple outcomes within multiple arms within studies we can also estimate this relationship using a multilevel mixed-effects meta-regression model similar to that applied above. In this case though we are including an additional *predictor* variable, the $\textrm{ln}\overline{x}$. For example, the following model specifies $\textrm{ln}\overline{x}$ as a fixed effect with random intercepts for study, arm, and effect:

\begin{equation}
\textrm{ln}\hat\sigma_{ijk}=(\beta_{0}+\tau_{(1)i}+\tau_{(2)j}+\tau_{(3)k})+\beta_{1}\textrm{ln}\overline{x}_{ijk}+\epsilon_{ijk}+m_{ijk} 
(\#eq:lnm-lns-model-eq)
\end{equation}

where $\textrm{ln}\hat\sigma_{ijk}$ is the $k\textrm{th}$ effect size, as in equation \@ref(eq:lnsigma-eq), from the $j\textrm{th}$ arm ($j = 1,2,\cdots,N_{j}$; where $N_{j}$ is the number of arms[^18]) in the $i\textrm{th}$ study ($i = 1,2,\cdots,N_{i}$; where $N_{i}$ is the number of studies), $\textrm{ln}\overline{x}_{ijk}$ is the mean estimate for each effect size, $\beta_{0}$ is the intercept or overall mean of the effects, $\beta_{1}$ is the slope or regression coefficient for $\textrm{ln}\overline{x}$, $\tau_{i}$ is the deviation from $\beta_{0}$ for the $i\textrm{th}$ study,  $\tau_{j}$ is the deviation for the $j\textrm{th}$ arm, $\tau_{k}$ is the deviation for the $k\textrm{th}$ effect, $\epsilon_{ijk}$ is the residual for each effect size which is normally distributed with $\sigma^2_{\epsilon}$, and $m_{ijk}$ is the sampling error for each effect size normally distributed with $\sigma^2_{\textrm{ln}\hat\sigma_{ijk}}$. 

These kinds of models are highly flexible. Additional predictor terms could be added; for example, we could model a categorical variable for the outcome type and include $\beta_{2}Outcome$ in the model with $Outcome$ as a dummy coded variable for the outcome type (i.e., hypertrophy = 0, and strength = 1), where $\beta_{2}$ is the slope or regression coefficient for $Outcome$ (most intuitively thought of as the difference between the two outcome types)[^19]. 

[^18]: In contrast to the models presented examining effect sizes relating to comparisons between and intervention group and control group, in the models examining $\textrm{ln}\sigma_{ijk}$ and $\textrm{ln}\overline{x}_{ijk}$ as a predictor the term *arm* refers to both the intervention groups(s) and control group. Thus, where a study had for example two RT interventions and a control, three separate arms would be coded (RT intervention 1, RT intervention 2, and control). Data were again coded such that study and arm had explicit nesting. 

[^19]: We do not have to limit ourselves to only fixed effect predictor terms as we have here. Indeed, for mixed effect models generally some argue that models should use a *maximal* random effects structure including both random intercepts and slopes (i.e., that the effect of the predictor term can vary within different levels of the model and is also assumed to come from an overarching distribution of slopes), and their correlations, to enhance generalisability of inferences [@barrRandomEffectsStructure2013]. We could model a categorical variable for the outcome type and using random effects include $(\beta_{2} + \varphi_{i})Outcome$ or $(\beta_{2} + \varphi_{i} + \varphi_{j})Outcome$ in the model with $Outcome$ as a dummy coded variable for the outcome type (i.e., hypertrophy = 0, and strength = 1), where $\beta_{2}$ is the overall average slope or regression coefficient for $Outcome$, and $\varphi_{i}$ is the deviation (random slope) from $\beta_{2}$ for the $i\textrm{th}$ study and $\varphi_{j}$ is the deviation for the $j\textrm{th}$ arm. These model specifications do not assume that the difference between outcomes is fixed, but can vary between studies and arms. We could also do the same and include random slopes for $(\beta_{1}$ on $\textrm{ln}\overline{x}$ thus allowing for the strength of the relationship between $\textrm{ln}\hat\sigma$ and $\textrm{ln}\overline{x}$ to also vary between studies and arms. Indeed, we fit a range of models  using $\textrm{ln}\hat\sigma$ with $\textrm{ln}\overline{x}$ and $Outcome$ as a predictor with (1) random intercepts only for study and arm, (2) the inclusion of correlated random slopes for $\textrm{ln}\overline{x}$ by study, (3) the inclusion of correlated random slopes for $\textrm{ln}\overline{x}$ by study and arm, (4) the inclusion of correlated random slopes for $Outcome$ by study, (5) the inclusion of correlated random slopes for $Outcome$ by study and arm, and (6) the inclusion of correlated random slopes for both $\textrm{ln}\overline{x}$ and $Outcome$ by study, (7) the inclusion of correlated random slopes for both $\textrm{ln}\overline{x}$ and $Outcome$ by study and arm.  The comparison of these models using $2\times\textrm{log}BF$ [@kassBayesFactors1995] from approximate Bayesian information criterion [@wagenmakersPracticalSolutionPervasive2007] to determine under which is the observed data most likely is included in the supplementary materials (https://osf.io/3tv6x). There was very strong evidence supporting the random intercepts only model compared to all others and so this is presented here.

Figure \@ref(fig:model-mean-variance-pre-plot) shows this model fit visually where the size of the points reflects their weight in the model. Both strength and hypertrophy outcomes show strong linearity between the mean and standard deviation on the log scale, though there is a small difference in intercepts between the two outcome types suggesting a slight but systematically greater degree of variance in strength measures compared to hypertrophy for a given mean score. 

```{r model-mean-variance-pre-plot, message=FALSE, warning=FALSE, echo=FALSE,fig.height=4, fig.width=6, fig.cap="Meta-analytic scatter plot of the log mean and log standard deviation of pre-intervention scores."}

# "Meta-analytic scatter plot of the log mean and log standard deviation of pre-intervention scores. Note, fitted lines are the regression slopes for strength and hypertophy outcomes each with their 95% confidence interval band."

load(here("plots/model_mean_variance_pre_plots"))

model_mean_variance_pre_plots

```

The presence of Taylor's law type relationships should be examined in datasets prior to deciding on which variance effect size statistic should be employed. Returning to the context of interindividual response variation to interventions, the presence of a mean-variance relationship in the data would imply that we cannot rely on absolute comparisons of variance (i.e., $SD_{ir}$ or $\textrm{ln}VR$) to determine whether interindividual response variation is actually present. So we should also explore this assumption for the change-scores in the RT and control groups and determining the appropriate effect sizes to explore.

## Reanalysis of interindividual response variation using $\textrm{ln}CVR$ 
As can be seen in figures \@ref(fig:mean-variance-delta-plot)(A) and (C) there is also a mean-variance relationship in the change score data about zero whereby an increase in the mean alone (i.e., greater mean change score in the intervention compared to the control) may be fully responsible for any apparent increase in variation. Further, when transforming change scores to absolute changes (i.e., converting all to positive numeric scores) we see that in figures \@ref(fig:mean-variance-delta-plot)(B) and (D) that the log transformation exhibits similar linearity as seen with the pre-intervention scores above. As such, in this case, we cannot rely solely on absolute comparisons of variance such as the $SD_{ir}$ and $\textrm{ln}VR$ to determine whether interindividual response variation is actually present. 

```{r mean-variance-delta-plot, message=FALSE, warning=FALSE, echo=FALSE,fig.height=8, fig.width=10, fig.cap="Scatter plots of raw mean and standard deviation of change scores for (A) strength outcomes and (B) hypertrophy outcomes, and of the log mean and log standard deviation of change scores for (C) strength outcomes and (D) hypertrophy outcomes."}

load(here("plots/mean_variance_delta_plots"))

mean_variance_delta_plots

```

The $\textrm{ln}CVR$ can be used to overcome this issue though. Fitting the same multilevel mixed-effects meta-analysis model with cluster-robust variance estimation and random intercepts for study, arm, and effect as before (see equation \@ref(eq:ml-model-eq)) using the $\textrm{ln}CVR$ as the effect size statistic leads to different conclusions compared to absolute variance comparisons using $SD_{ir}$ or $\textrm{ln}VR$. The introduction of an RT intervention actually *reduces* the relative variation seen in change scores for strength ($\textrm{ln}CVR$ = `r mods_SMD_logCVR$Estimate[28]` [95%CI: `r mods_SMD_logCVR$Lower[28]` to `r mods_SMD_logCVR$Upper[28]`]; $I^2_{study}$ = `r mods_SMD_logCVR$I.2.study[28]`%, $I^2_{arm}$ = `r mods_SMD_logCVR$I.2.arm[28]`%, $I^2_{effect}$ = `r mods_SMD_logCVR$I.2.effect[28]`%) and hypertrophy ($\textrm{ln}CVR$ = `r mods_SMD_logCVR$Estimate[88]` [95%CI: `r mods_SMD_logCVR$Lower[88]` to `r mods_SMD_logCVR$Upper[88]`]; $I^2_{study}$ = `r mods_SMD_logCVR$I.2.study[88]`%, $I^2_{arm}$ = `r mods_SMD_logCVR$I.2.arm[88]`%, $I^2_{effect}$ = `r mods_SMD_logCVR$I.2.effect[88]`%) and further there is lower relative heterogeneity between studies in the effect estimates (figure \@ref(fig:forest-logCVR-plot)).

```{r forest-logCVR-plot, message=FALSE, warning=FALSE, echo=FALSE, fig.height=10, fig.width=10, fig.cap="Caterpillar plots of ln$CVR$ effect sizes for strength (A) and hypertrophy (B) outcomes."}

# "Caterpillar plots of $\textrm{ln}CVR$ effect sizes for strength and hypertrophy outcomes. Note, each estimate and 95% confidence interval is a single observed effect size within each arm within each study, the diamond reflects the model overall estimate and 95% confidence interval, and the horizontal line with vertical bars at the end reflect the 95% prediction interval"

load(here("plots/forest_logCVR_strength"))
load(here("plots/forest_logCVR_hypertrophy"))
library(patchwork)
(forest_logCVR_strength / forest_logCVR_hypertrophy) +
  plot_annotation(tag_levels = "A")

```

There is, however, a potential limitation for the $\textrm{ln}CVR$ also that may need to be considered. Firstly, it is limited to the use of ratio scale data (which is not the case for the $\textrm{ln}\hat\sigma$ or $\textrm{ln}VR$); hence the need to transform the change scores to be positively signed in this specific case. Secondly, whilst the $\textrm{ln}CVR$ is useful in situations where there is a mean-variance relationship, the use of the $CV$ in the effect size statistic assumes proportionality between standard deviation and mean. Where we see the kind of heteroskedasticity in the relationship between mean and standard deviation as we do for the change scores here (figure \@ref(fig:mean-variance-delta-plot)) an alternative yet comparable approach might be desirable. Lastly, this statistic is limited to examination of pairwise comparisons of variance. In this example this approach poses no issue as we are comparing RT intervention group(s) to a control group. But, as seen with the pre-score example above, alternative modelling approaches provide greater flexibility and can still offer estimation of a comparable effect of interest to the $\textrm{ln}CVR$ one just applied. 

## Meta-regression of $\textrm{ln}\hat\sigma$ with $\textrm{ln}\overline{x}$ and $Group$

```{r, message=FALSE,warning=FALSE,echo=FALSE}
# load the summary dataframes to the environment
load(here("models/RobuEstMultiLevelModel_ri_only_log_mean_mod_strength"))
load(here("models/RobuEstMultiLevelModel_ri_only_log_mean_mod_hypertrophy"))

```

Instead, we can use a version of the meta-regression model described above (see equation \@ref(eq:lnm-lns-model-eq) and the paragraph which followed it) to compare the variability in change scores between intervention and control groups using $\textrm{ln}\hat\sigma$ and $\textrm{ln}\overline{x}$. In this case, the categorical variable for the outcome type used previously is instead swapped for the group type and the new model term included becomes $\beta_{2}Group$ with $Group$ as a dummy coded variable for the group (i.e., non-training control = 0, and RT intervention = 1), where $\beta_{2}$ is the slope or regression coefficient for $Group$. This kind of meta-regression model with  $Group$ as a predictor is comparable to the $\textrm{ln}CVR$ model in the previous section where the $\beta_{2}Group$ is the slope or regression coefficient for $Group$ and reflects the difference i.e., variance in the RT groups vs the control groups. This reflects the pairwise nature of the $\textrm{ln}CVR$. 

```{r model-mean-variance-delta-plot, message=FALSE, warning=FALSE, echo=FALSE,fig.height=4, fig.width=8, fig.cap="Meta-analytic scatter plot of the log mean and log standard deviation of change scores."}

# "Meta-analytic scatter plot of the log mean and log standard deviation of change scores. Note, fitted lines are the regression slopes for control group (CON) and resistance training intervention (RT) groups each with their 95% confidence interval band."

load(here("plots/model_mean_variance_delta_plots"))

model_mean_variance_delta_plots

```

Given the heteroskedasticity in the change scores means and standard deviations (see figure \@ref(fig:mean-variance-delta-plot)), we fit this model and other model specifications[^20]\textsuperscript{,}[^21] to the dataset (see figure \@ref(fig:model-mean-variance-delta-plot)). The results were largely similar to those found using the $\textrm{ln}CVR$ model for strength ($\beta_{\textrm{ln}\hat\sigma[Group \textrm{ for RT}]}$ = `r round(RobuEstMultiLevelModel_ri_only_log_mean_mod_strength$b[3],2)` [95%CI: `r round(RobuEstMultiLevelModel_ri_only_log_mean_mod_strength$ci.lb[3],2)` to `r round(RobuEstMultiLevelModel_ri_only_log_mean_mod_strength$ci.ub[3],2)`]) and hypertrophy ($\beta_{\textrm{ln}\hat\sigma[Group \textrm{ for RT}]}$ = `r round(RobuEstMultiLevelModel_ri_only_log_mean_mod_hypertrophy$b[3],2)` [95%CI: `r round(RobuEstMultiLevelModel_ri_only_log_mean_mod_hypertrophy$ci.lb[3],2)` to `r round(RobuEstMultiLevelModel_ri_only_log_mean_mod_hypertrophy$ci.ub[3],2)`]). Both models suggest that the introduction of the RT interventions *reduced* variance in change scores.

[^20]: Note, as with the models examining $Outcome$ upon baseline scores, we similarly explored $\textrm{ln}\hat\sigma$ with $\textrm{ln}\overline{x}$ and $Group$ as a predictor with (1) random intercepts only for study and arm, (2) the inclusion of correlated random slopes for $\textrm{ln}\overline{x}$ by study, (3) the inclusion of correlated random slopes for $\textrm{ln}\overline{x}$ by study and arm, (4) the inclusion of correlated random slopes for $Group$ by study, (5) the inclusion of correlated random slopes for both $\textrm{ln}\overline{x}$ and $Group$ by study, and (6) the inclusion of correlated random slopes for both $\textrm{ln}\overline{x}$ and $Outcome$ by study and $\textrm{ln}\overline{x}$ by arm (we do not include the models with random slopes for $Group$ by arm as in this model each arm refers to a particular group, RT or CON, and so no arm provides data for both).  The comparison of these models using $2\times\textrm{log}BF$ [@kassBayesFactors1995] from approximate Bayesian information criterion [@wagenmakersPracticalSolutionPervasive2007] to determine under which is the observed data most likely is included in the supplementary materials (see https://osf.io/b5deh for strength and https://osf.io/5ektd for hypertrophy). Similar to the models including $Outcome$ there was very strong evidence supporting the random intercepts only model compared to all others and so this is presented here. All estimates for the difference between RT and CON, where positive values indicate RT *increased* variation in changes scores and negative values indicate it *decreased* variation, can be seen in the supplementary materials (https://osf.io/5g7ce) all of which revealed similar conclusions.

[^21]: It is perhaps worth explaining the assumptions that the different models explored make regarding the mean-variance relationship. For example, the $\textrm{ln}VR$ and $\textrm{ln}CVR$ models can be thought of as similar in that they both make fixed assumptions about the relationship between mean and variance; in the $\textrm{ln}VR$ it is assumed to be zero, and in the $\textrm{ln}CVR$ it is assumed to be proportional i.e., one. In both however this is a *strong* assumption. The multilevel meta-regressions on the other hand actually *estimate* this relationship (i.e., the value of $\beta_{1}$, the slope or regression coefficient for $\textrm{ln}\overline{x}$) and in model where random slopes are included this is also estimated allowing it to vary between studies and arms (i.e., in some studies there may be a more or less strong relationship compared to others). Mean-variance relationships are important to consider when exploring variance effects, but it is also important to consider whether or not this relationship is assumed to be some fixed proportional value (i.e., as the $\textrm{ln}CVR$ does) or whether or not this should be estimated from the data and whether it might also vary across studies and arms (i.e., as the multilevel meta-regression models allow). It should also be noted that these models all assume that the $\overline{x}$ is estimated without error which is clearly not the case. Given that for most effects that might be included in such models we can determine the sampling variance for $\overline{x}$ one approach to address this might be to employ models that incorporate the variance on this predictor (i.e., measurement error or errors in variables models). This is beyond the scope of this paper to discuss. It is not necessarily clear which model should be preferred here, and fortunately substantive conclusions are impacted little by model specification, but thought should be given to the assumptions each makes and the fit of each model to the data.

## Moderators of variance effects
Hopefully it is clear from the meta-regression models here, where we have included both fixed and random predictors as both categorical (i.e., $Outcome$, or $Group$) and continuous (i.e., $\textrm{ln}\overline{x}$) variables, that there is considerable flexibility in the inclusion of predictors when exploring variance through a meta-analytic framework. Of course, the pairwise models described can also be extended to meta-regressions to explore not only how study, arm, or effect level characteristics moderate effect size estimates when considering effect sizes such as SMDs or $\textrm{ln}RR$, but also when considering the variance-based effect size statistics and models employed in this article (i.e., $SD_{ir}$, $\textrm{ln}VR$, $\textrm{ln}CVR$, and $\textrm{ln}\hat\sigma$)[^22]. But these are limited to the pairwise comparisons of a categorical variable as the effect size. The meta-regression models presented here for $\textrm{ln}\hat\sigma$ allow for comparisons to be extended beyond two categories (e.g., intervention, placebo, and control) including any number of arbitrary predictors, fixed and random and assumptions about their correlations.

[^22]: See supplementary materials (https://osf.io/e6vpr) for examples from model estimates for both SMD and $\textrm{ln}CVR$, (used for simplicity of presenting moderator analysis results) across a range of categorical and continuous predictors for both strength and hypertrophy outcomes. There were no obvious moderators of $\textrm{ln}CVR$ in particular.

# Discussion
Given the apparent lack of awareness of the utility of meta-analytic frameworks for exploring variance, and the potential value such analyses can offer for the sport and exercise sciences, we have presented some existing effect size statistics and models pertinent to this topic that hopefully will encourage and support researchers in the field to embrace more than just the mean when engaging in quantitative evidence synthesis. Indeed, for a field such as sport and exercise science where sample sizes are typically small, meta-analysis becomes even more valuable as such small samples in primary studies have even lower statistical power to detect differences in variation as compared to means[^23] [@yangLowStatisticalPower2022]. 

[^23]: Indeed, it can be seen from figures \@ref(fig:forest-SDir-plot), \@ref(fig:forest-logVR-plot), and \@ref(fig:forest-logCVR-plot) that many of the individual study effect estimates have very large sampling errors.

It is of particular interest to note the different conclusions drawn here are dependent on the approach taken to determine from non-training control and RT intervention data whether or not there is *detectable* inter-individual response variation present. We deliberately presented these varying approaches to highlight their assumptions and to aid in readers understanding of their applications. Given the different conclusions drawn from the examples provided, we recommend that researchers consider whether assumptions of simpler approaches are met, or indeed seem reasonable, before their application in specific situations. Where for example there is not an obvious mean-variance relationship it may be appropriate to utilise the $SD_{ir}$ or $\textrm{ln}VR$. However, when this is present in the data then the $\textrm{ln}CVR$ or meta-regression of $\textrm{ln}\hat\sigma$ upon $\textrm{ln}\overline{x}$ may be more appropriate, though assumptions regarding whether this relationship is proportional and fixed should also be considered. The structure of the data also impacts the specific modelling approach to be employed and whether or not assumptions that the mean-variance relationship varies within study or arm levels can be incorporated. Further, depending on the exact research questions it is worth considering balancing model complexity with its ability to provide an answer. If a simple pairwise comparison across a categorical variable is of interest then this can be explored with comparable models using the $\textrm{ln}CVR$ or meta-regression of $\textrm{ln}\hat\sigma$ upon $\textrm{ln}\overline{x}$. But where more complicated predictors, including categories extending beyond just two, are of interest then the flexibility of the latter is desirable to explore. Lastly, it is possible that substantive conclusions might be sensitive to the exact model specifications used. As such, it is worth considering the use of so called *multiverse* approaches to exploring questions such as those pertaining to variation with meta-analysis. With multiverse approaches, instead of having to choose one modelling approach, researchers use the many possible approaches (such as presented here) and explore how sensitive their substantive conclusions might be to nuances of model specifications and assumptions [@steegenIncreasingTransparencyMultiverse2016; @olsson-collentineMetaanalyzingMultiversePeek2023]. 

The examples presented herein used data from RT studies included in a recent meta-analysis published in the *Journal of Sport Sciences* [@politoModeratorsStrengthGains2021], which hopefully makes them more relatable for researchers in sport and exercise sciences. Examining this data using absolute comparisons of variance such as $SD_{ir}$ and $\textrm{ln}VR$ gave the impression that the introduction of the RT intervention likely *increased* variance above random error, suggesting the presence of inter-individual response variation. In the case of RT interventions there is evidently an average intervention effect for strength and hypertrophy which is positive (as seen in the SMD and $\textrm{ln}RR$ models), yet combined with the results from $SD_{ir}$ and $\textrm{ln}VR$ we might conclude that while all likely benefit, some benefit more so than others. Indeed, even if for an intervention there was not clear evidence for average intervention effects, exploring variation in their absence might still be important as interventions with large enough variance could imply that the intervention is at least beneficial to some [@usuiMetaanalysisVariationSuggests2021]. Such results might lead researchers to consider that further research exploring subgroup- or participant-by-intervention interactions is required to maximise successful practical application of such an intervention to avoid negative effects for some, and ensure positive effects for others.

However, similar to the cross-sectional pre-intervention score models reported here (and indeed most physical and biological variables), change scores also demonstrated a mean-variance relationship in addition to heteroskedasticity. The likely more appropriate analyses in this case using the $\textrm{ln}CVR$ or meta-regression of $\textrm{ln}\hat\sigma$ upon $\textrm{ln}\overline{x}$ revealed conclusions in the opposite substantive direction of the absolute variance comparisons; essentially, that the introduction of the RT intervention may have slightly *decreased* change score variance, implying that there is likely little to no interindividual response variation to explain. Interventions, such as RT interventions explored here, which induce both meaningful average treatment effects and also show little evidence suggestive of interindividual variation, are likely to be widely generalisable and so from a practical perspective might offer considerable value in that we can have high expectations that everyone receiving them will likely improve [@usuiMetaanalysisVariationSuggests2021]; that is to say we can assume a constant effect and that the average intervention effect is indicative of the individual intervention effect [@cortesmartinezConstantEffectRandomized2021]. Interventions such as these are valuable for the simplification of guidelines and recommendations. For example, muscle strengthening interventions such as RT are recommended for *everyone* in current physical activity guidelines and in such applications there is likely value in a simple approach to such recommendations [@steeleHigherEffortbasedParadigm2017; @steeleLongTermTimeCourseStrength2022]. 

The reason for the apparent reduction in variation after introduction of an RT intervention observed here is not necessarily discernible from this analysis. Perhaps the introduction of an RT intervention has indirect effects that reduce other sources of random variance (e.g., diet, other physical activity etc.; @hallidayResistanceTrainingAssociated2017), or a ceiling effect on change (i.e., plateau in response; @steeleLongTermTimeCourseStrength2022) has a constraining effect [@cortesmartinezConstantEffectRandomized2021]. However, this potentially represents another interesting area of future study regarding variation; specifically, how to produce interventions that actually reduce variance in an outcome. In other contexts such as sporting performance, interventions to not only positively affect mean performance but also those that reduce variation in performance would be highly desirable.  


# Conclusion
Embracing variability and focusing on more than merely the mean differences between groups or conditions, such as intervention and control comparisons, has the potential to inform experimental design and lead to changes in both the approach and direction of follow-up studies. Whether there is evidence of meaningful average intervention effects or not, where considerable variance effects are present it suggests that a meaningful line of research would be to aim at identifying subgroup- or participant-by-intervention interactions using appropriate study designs [@heckstedenIndividualResponseExercise2015]. Where variance effects are limited this instead suggests that translational work towards generalisable implementation might be the most meaningful line of future research. Finally, there may be cases where it is in fact desirable to identify interventions that actually reduce variance; for example, improvements in methodological approaches to enhance research [@usuiMetaanalysisVariationSuggests2021], or interventions to reduce variation in sport performances. Thus, researchers in sport and exercise science should consider exploring variance more systematically, and indeed utilise the meta-analytic framework to support this. This could include the re-analysis of past meta-analyses as we have done here, and indeed researchers conducting future meta-analyses in the field of sport and exercise science should consider the value of concomitantly exploring means and variances utilising the established approaches [@nakagawaMetaanalysisVariationEcological2015; @hopkinsIndividualResponsesMade2015; @atkinsonTrueFalseInterindividual2015; @atkinsonIssuesDeterminationResponders2019; @usuiMetaanalysisVariationSuggests2021; @millsDetectingHeterogeneityIntervention2021] presented here and echoing the efforts of other recent work [@kelleyAreThereInterindividual2022; @kelleyAreThereInterIndividual2020; @estevesIndividualParticipantData2021; @bonafigliaInterindividualDifferencesTrainability2022; @steeleSlowSteadyHard2021; @fisherRoleSupervisionResistance2022]. Researchers should be aware though that the meta-analytic modelling of variance, or indeed any effect size statistics, requires careful consideration of the research question and assumptions that different models make. Substantive conclusions may be sensitive to subtle differences in the approaches we have presented.


# References

<div id="refs"></div>
